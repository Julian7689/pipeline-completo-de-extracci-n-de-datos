# ğŸš€ Data Pipeline - Web Scraping Project

Una aplicaciÃ³n web Flask moderna y elegante para extraer, procesar y visualizar datos de sitios web mediante web scraping.

## ğŸ“‹ DescripciÃ³n

Este proyecto es un pipeline completo de extracciÃ³n de datos (web scraping) que permite:

- **AutenticaciÃ³n segura**: Sistema de login para acceder a sitios web protegidos
- **ExtracciÃ³n automatizada**: Scraping de datos de quotes.toscrape.com
- **Procesamiento de datos**: Limpieza y estructuraciÃ³n de informaciÃ³n extraÃ­da
- **MÃºltiples formatos**: ExportaciÃ³n automÃ¡tica a Excel, CSV y JSON
- **Interfaz visual**: Dashboard interactivo para gestionar el proceso completo

## âœ¨ CaracterÃ­sticas

- ğŸ¨ **Interfaz moderna**: DiseÃ±o con gradientes vibrantes y animaciones suaves
- ğŸ” **AutenticaciÃ³n**: Sistema de login con sesiones seguras
- ğŸ“Š **VisualizaciÃ³n de datos**: Tablas interactivas con estadÃ­sticas
- ğŸ’¾ **ExportaciÃ³n mÃºltiple**: Guarda datos en 3 formatos diferentes simultÃ¡neamente
- ğŸš€ **Procesamiento asÃ­ncrono**: Feedback en tiempo real durante el scraping
- ğŸ“± **Responsive**: DiseÃ±o adaptable a diferentes tamaÃ±os de pantalla

## ğŸ› ï¸ TecnologÃ­as Utilizadas

### Backend
- **Flask**: Framework web de Python
- **Requests**: Para realizar peticiones HTTP
- **BeautifulSoup4**: Para parsear y extraer datos HTML
- **Pandas**: Para procesamiento y exportaciÃ³n de datos

### Frontend
- **HTML5**: Estructura semÃ¡ntica
- **CSS3**: Estilos modernos con gradientes y animaciones
- **JavaScript**: Interactividad y llamadas asÃ­ncronas
- **Google Fonts (Inter)**: TipografÃ­a moderna

## ğŸ“ Estructura del Proyecto

```
Proyecto_Pipeline/
â”‚
â”œâ”€â”€ app.py                 # AplicaciÃ³n Flask principal
â”œâ”€â”€ templates/             # Plantillas HTML
â”‚   â”œâ”€â”€ login.html        # PÃ¡gina de inicio de sesiÃ³n
â”‚   â”œâ”€â”€ index.html        # Dashboard principal
â”‚   â””â”€â”€ results.html      # VisualizaciÃ³n de resultados
â”‚
â”œâ”€â”€ static/               # MÃ³dulos Python del pipeline
â”‚   â”œâ”€â”€ login.py         # LÃ³gica de autenticaciÃ³n
â”‚   â”œâ”€â”€ scrapper.py      # ExtracciÃ³n de datos
â”‚   â”œâ”€â”€ processor.py     # Procesamiento y exportaciÃ³n
â”‚   â””â”€â”€ main.py          # Pipeline completo (CLI)
â”‚
â””â”€â”€ README.md            # Este archivo
```

## ğŸš€ InstalaciÃ³n

### Requisitos Previos

- Python 3.7 o superior
- pip (gestor de paquetes de Python)

### Pasos de InstalaciÃ³n

1. **Clonar o descargar el proyecto**
   ```bash
   cd Proyecto_Pipeline
   ```

2. **Instalar dependencias**
   ```bash
   pip install flask requests beautifulsoup4 pandas openpyxl
   ```

3. **Ejecutar la aplicaciÃ³n**
   ```bash
   python app.py
   ```

4. **Acceder a la aplicaciÃ³n**
   
   Abre tu navegador y visita: `http://127.0.0.1:5000`

## ğŸ“– Uso

### 1. Inicio de SesiÃ³n

- Accede a la pÃ¡gina de login
- Ingresa tus credenciales
- El sistema autenticarÃ¡ tu sesiÃ³n con el sitio objetivo

### 2. Dashboard Principal

Una vez autenticado, verÃ¡s tres tarjetas principales:

- **ğŸ“Š Iniciar Scraping**: Ejecuta el proceso de extracciÃ³n de datos
- **ğŸ“ Ver Resultados**: Visualiza los datos extraÃ­dos en una tabla
- **ğŸ’¾ Archivos Generados**: InformaciÃ³n sobre los archivos exportados

### 3. Ejecutar Scraping

1. Haz clic en "Ejecutar Scraping"
2. El sistema mostrarÃ¡ un indicador de carga
3. Una vez completado, serÃ¡s redirigido a la pÃ¡gina de resultados

### 4. Visualizar Resultados

La pÃ¡gina de resultados muestra:

- **EstadÃ­sticas**: NÃºmero de citas extraÃ­das, autores Ãºnicos, formatos guardados
- **Tabla de datos**: InformaciÃ³n completa de cada cita extraÃ­da
  - Texto de la cita
  - Autor
  - TÃ­tulo
  - DescripciÃ³n

### 5. Archivos Generados

Los datos se guardan automÃ¡ticamente en la raÃ­z del proyecto:

- `data001.xlsx` - Formato Excel
- `data001.csv` - Formato CSV
- `data001.json` - Formato JSON

## ğŸ”§ MÃ³dulos del Pipeline

### `login.py`
Gestiona la autenticaciÃ³n con el sitio web objetivo.

```python
logeado_credenciales(session, username, password)
```
- Realiza login en quotes.toscrape.com
- Mantiene la sesiÃ³n activa
- Retorna True si el login es exitoso

### `scrapper.py`
Extrae datos del sitio web.

```python
scrape_data(session)
```
- Obtiene las citas de la pÃ¡gina
- Extrae: texto, autor, tÃ­tulo y descripciÃ³n
- Retorna una lista de diccionarios con los datos

### `processor.py`
Procesa y exporta los datos.

```python
save_to_excel(data, filename)
save_to_csv(data, filename)
save_to_json(data, filename)
```
- Convierte datos a DataFrame de Pandas
- Exporta en mÃºltiples formatos
- Guarda archivos en el directorio actual

### `main.py`
Pipeline completo para uso desde lÃ­nea de comandos.

```python
pipeline(username, password)
```
- Ejecuta todo el proceso de forma secuencial
- Ãštil para automatizaciÃ³n sin interfaz web

## ğŸ¨ CaracterÃ­sticas de DiseÃ±o

- **Gradientes vibrantes**: Paleta de colores moderna (pÃºrpura, azul)
- **Glassmorphism**: Efectos de vidrio esmerilado en tarjetas
- **Animaciones suaves**: Transiciones y efectos hover
- **TipografÃ­a moderna**: Fuente Inter de Google Fonts
- **Feedback visual**: Indicadores de carga y mensajes flash
- **DiseÃ±o responsive**: Adaptable a mÃ³viles y tablets

## ğŸ” Seguridad

- **Sesiones Flask**: GestiÃ³n segura de sesiones de usuario
- **Secret Key**: Clave secreta para firmar cookies (cambiar en producciÃ³n)
- **ValidaciÃ³n de autenticaciÃ³n**: ProtecciÃ³n de rutas sensibles
- **Manejo de errores**: Mensajes informativos sin exponer detalles internos

## ğŸ“Š Datos ExtraÃ­dos

El scraper extrae la siguiente informaciÃ³n de cada cita:

| Campo | DescripciÃ³n |
|-------|-------------|
| **Text** | Texto completo de la cita |
| **Author** | Nombre del autor de la cita |
| **Title** | TÃ­tulo de la pÃ¡gina (si existe) |
| **Description** | Meta descripciÃ³n del sitio |

## ğŸš§ Uso desde CLI (LÃ­nea de Comandos)

TambiÃ©n puedes ejecutar el pipeline sin la interfaz web:

```bash
cd static
python main.py
```

Esto ejecutarÃ¡ el proceso completo y guardarÃ¡ los archivos automÃ¡ticamente.

## ğŸ¤ Contribuciones

Este es un proyecto educativo de anÃ¡lisis de datos. SiÃ©ntete libre de:

- Reportar bugs
- Sugerir mejoras
- Agregar nuevas funcionalidades
- Mejorar la documentaciÃ³n

## âš ï¸ Notas Importantes

1. **Uso Responsable**: Este proyecto es para fines educativos. Respeta los tÃ©rminos de servicio de los sitios web.
2. **Rate Limiting**: No hagas scraping excesivo que pueda sobrecargar los servidores.
3. **Robots.txt**: Verifica siempre el archivo robots.txt del sitio objetivo.
4. **Credenciales**: Las credenciales en `login.py` son de ejemplo. Usa las tuyas propias.

## ğŸ“ Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible para fines educativos.

## ğŸ‘¨â€ğŸ’» Autor

Proyecto desarrollado como parte de un pipeline de anÃ¡lisis de datos.

---

**Â¿Preguntas o sugerencias?** No dudes en abrir un issue o contactar al desarrollador.

ğŸŒŸ **Â¡Gracias por usar Data Pipeline!** ğŸŒŸ
